{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb8-ympJ26uF"
      },
      "source": [
        "# INCS 615 Advanced Network and Internet Security\n",
        "#### Spring 2025, INCS 615-VA1 (2854)\n",
        "#### Instructor: Dr. Zhida Li\n",
        "#### Email: zli74@nyit.edu\n",
        "\n",
        "## Lab #3 (Group) - Detecting Intrusions using Machine Learning Models\n",
        "### Please keep your output when you submit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQsXszMj26uL"
      },
      "source": [
        "# Step 1: Load the data, learn the five classes, and count the number of data points for each class\n",
        "Write the Python code:\n",
        "- Load the training and testing data\n",
        "- Count the number of data points for Regular (0), DoS (1), R2L (2), U2R (3), Probe (4) in both training and testing datasets\n",
        "\n",
        "You may use pandas OR numpy for extraction and removing the header:\n",
        "- [_NumPy_](https://numpy.org): used to perform mathematical operations\n",
        "- [_pandas_](https://pandas.pydata.org/): open source data analysis and manipulation tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqZbcikq2jbH",
        "outputId": "0e64013d-e70a-4336-8efa-2bd74290f756"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# To do...\n",
        "# Load your NSL-KDD training dataset\n",
        "#data_train = pd.read_csv('KDDTrain+_20Percent_615.csv', header=None)\n",
        "data_train = pd.read_csv('KDDTrain+_20Percent_615.csv')  # Keep the header\n",
        "\n",
        "# Load your NSL-KDD testing dataset\n",
        "data_test = pd.read_csv('KDDTest+_615.csv')\n",
        "#data_test = pd.read_csv('KDDTest+_615.csv', header=None)\n",
        "# ...\n",
        "\n",
        "\n",
        "# To do...\n",
        "# Count the number of data points for Regular (0), DoS (1), R2L (2), U2R (3), Probe (4) in both training and testing datasets\n",
        "# training dataset\n",
        "# Count the number of data points for each class in the training dataset\n",
        "num_regular = data_train[data_train.iloc[:, -1] == 0].shape[0]\n",
        "num_dos = data_train[data_train.iloc[:, -1] == 1].shape[0]\n",
        "num_r2l = data_train[data_train.iloc[:, -1] == 2].shape[0]\n",
        "num_u2r = data_train[data_train.iloc[:, -1] == 3].shape[0]\n",
        "num_probe = data_train[data_train.iloc[:, -1] == 4].shape[0]\n",
        "print('\\Training Dataset:')\n",
        "print('Regular data points:', num_regular)\n",
        "print('DoS data points:', num_dos)\n",
        "print('R2L data points:', num_r2l)\n",
        "print('U2R data points:', num_u2r)\n",
        "print('Probe data points:', num_probe)\n",
        "\n",
        "# Count the number of data points for each class in the testing dataset\n",
        "num_regular_test = data_test[data_test.iloc[:, -1] == 0].shape[0]\n",
        "num_dos_test = data_test[data_test.iloc[:, -1] == 1].shape[0]\n",
        "num_r2l_test = data_test[data_test.iloc[:, -1] == 2].shape[0]\n",
        "num_u2r_test = data_test[data_test.iloc[:, -1] == 3].shape[0]\n",
        "num_probe_test = data_test[data_test.iloc[:, -1] == 4].shape[0]\n",
        "\n",
        "print('\\nTesting Dataset:')\n",
        "print('Regular data points:', num_regular_test)\n",
        "print('DoS data points:', num_dos_test)\n",
        "print('R2L data points:', num_r2l_test)\n",
        "print('U2R data points:', num_u2r_test)\n",
        "print('Probe data points:', num_probe_test)\n",
        "\n",
        "# Also testing dataset\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20q6D6wb2jbI"
      },
      "source": [
        "# Step 2: Prepare numerical features\n",
        "Write the Python code:\n",
        "- Select a method and Convert 3 categorical features (“protocol_type”, “service”, and “flag”) to numerical from both training and testing data.\n",
        "  (Data should be numerical when feeding into machine learning models.)\n",
        "\n",
        "You may use pandas OR numpy:\n",
        "- [_NumPy_](https://numpy.org): used to perform mathematical operations\n",
        "- [_pandas_](https://pandas.pydata.org/): open source data analysis and manipulation tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRB2wBUNuULf"
      },
      "outputs": [],
      "source": [
        "categorical_features = [\"protocol_type\", \"service\", \"flag\"]\n",
        "train_df = data_train.copy()\n",
        "test_df = data_test.copy()\n",
        "# n (next): Execute the next line.\n",
        "# s (step): Step into a function call.\n",
        "# c (continue): Continue execution until the next breakpoint or the end of the program.\n",
        "# p (print): Print the value of a variable.\n",
        "# q (quit): Exit the debugger.\n",
        "for feature in categorical_features:\n",
        "    #%debug\n",
        "\n",
        "     # Get column position using iloc\n",
        "    # try:\n",
        "    #     #import pdb; pdb.set_trace()\n",
        "    #     feature_position = train_df.columns.get_loc(feature)\n",
        "\n",
        "    # except KeyError:\n",
        "    #     print(f\"Feature '{feature}' not found, skipping...\")\n",
        "    #     continue  # Skip to the next feature\n",
        "\n",
        "    # # Generate dummy variables for the training data using iloc\n",
        "    # train_dummies = pd.get_dummies(train_df.iloc[:, feature_position], prefix=feature)\n",
        "\n",
        "    train_dummies = pd.get_dummies(train_df[feature], prefix=feature)\n",
        "\n",
        "\n",
        "    # Get the columns to ensure alignment with test data\n",
        "    feature_columns = train_dummies.columns\n",
        "\n",
        "    # Generate dummy variables for the test data\n",
        "    #test_dummies = pd.get_dummies(test_df.iloc[:, feature_position], prefix=feature)\n",
        "    test_dummies = pd.get_dummies(test_df[feature], prefix=feature)\n",
        "    # Reindex test dummy columns to match training columns, filling missing with 0\n",
        "    test_dummies = test_dummies.reindex(columns=feature_columns, fill_value=0)\n",
        "\n",
        "    train_df = pd.concat([train_df, train_dummies], axis=1)\n",
        "    test_df = pd.concat([test_df, test_dummies], axis=1)\n",
        "\n",
        "    # Drop the original categorical column from both datasets\n",
        "    #import pdb; pdb.set_trace()\n",
        "    train_df = train_df.drop(columns=[feature])\n",
        "    #import pdb; pdb.set_trace()\n",
        "    try:\n",
        "        #test_df = test_df.drop(feature, axis=1)\n",
        "        #import pdb; pdb.set_trace()\n",
        "        test_df = test_df.drop(columns=[feature])\n",
        "    except KeyError:\n",
        "        print(f\"Feature '{feature}' not found in test data, skipping...\")\n",
        "        continue  # Skip to the next feature\n",
        "    #test_df = test_df.drop(feature, axis=1)\n",
        "\n",
        "    # Concatenate the dummy variables to the datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzkcEbKo26uL"
      },
      "source": [
        "# Step 3: Nomalize the data and create ML models\n",
        "Write the Python code to:\n",
        "- Normalize the two datasets (training and test data);  \n",
        "- Run a ML model. Various ML algorithms are available in the ML library (https://scikit-learn.org/stable/index.html).\n",
        "\n",
        "If you are running the exercise on your local platform, download and install machine learning (ML) library:  \n",
        "\thttps://scikit-learn.org/stable/index.html\n",
        "\n",
        "The Python libraries installed by [_pip_](https://pip.pypa.io/en/stable/) are:\n",
        "- [_SciPy_](https://scipy.org): dependency of the _scikit-learn_ library.\n",
        "- _SciPy_'s _zscore_: function used to perform normalization.\n",
        "- [_scikit-learn_](https://scikit-learn.org/stable): employed for processing data and calculating performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtHzn7pj4ZWt",
        "outputId": "977b82e4-94a5-4899-93a4-73521eeac7cd"
      },
      "outputs": [],
      "source": [
        "# Import the Python libraries\n",
        "import time\n",
        "from scipy.stats import zscore\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier  # Assuming you're using RandomForest\n",
        "\n",
        "import numpy as np #import numpy\n",
        "\n",
        "# Get training and test data and labels for model training\n",
        "# Convert pandas DataFrames to NumPy arrays\n",
        "features_train = train_df.drop(columns=train_df.columns[-1]).values  # Exclude last column (target)\n",
        "labels_train = train_df.iloc[:, -1].values  # Target variable in the last column\n",
        "features_test = test_df.drop(columns=test_df.columns[-1]).values  # Exclude last column (target)\n",
        "labels_test = test_df.iloc[:, -1].values  # Target variable in the last column\n",
        "\n",
        "# Handle NaNs before applying zscore\n",
        "#import pdb; pdb.set_trace()\n",
        "features_train = np.nan_to_num(features_train).astype(np.float64)  # Replace NaNs, convert to float64\n",
        "features_test = np.nan_to_num(features_test).astype(np.float64)  # Replace NaNs, convert to float64\n",
        "\n",
        "# Normalize the training and test datasets using zscore\n",
        "features_train = zscore(features_train, axis=0, ddof=1)\n",
        "features_test = zscore(features_test, axis=0, ddof=1)\n",
        "\n",
        "# Create and train your model\n",
        "time_start = time.time()  # training time - start\n",
        "#model = DecisionTreeClassifier()  # Initialize the DecisionTreeClassifier\n",
        "# Initialize RandomForestClassifier with parameters\n",
        "model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "\n",
        "# Generate the model using training data and labels\n",
        "model.fit(features_train, labels_train)\n",
        "\n",
        "time_end = time.time()  # training time - end\n",
        "training_time = time_end - time_start\n",
        "print('Training completed')\n",
        "print('Training time:', training_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_nPfWai26uM"
      },
      "source": [
        "# Step 4:\n",
        "Write the Python code to:\n",
        "- Test the developed model on the test dataset named \"features_test\"\n",
        "- Calculate Accuracy and F1-Score based on test labels and predicted labels.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUGyqCR5Gu9O",
        "outputId": "8ea8cf1e-0891-4687-ea63-ecd8e0e3c359"
      },
      "outputs": [],
      "source": [
        "# Import the Python libraries\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Testing, for sklearn libriary if applicable\n",
        "predicted_labels = model.predict(features_test)\n",
        "\n",
        "# To do...\n",
        "# Performance metrics\n",
        "# accuracy = accuracy_score(labels_test, predicted_labels)\n",
        "# fscore = f1_score(labels_test, predicted_labels)\n",
        "# Okay, let's discuss the meaning of accuracy_score and f1_score in the context of your intrusion detection task.\n",
        "\n",
        "# Accuracy Score:\n",
        "\n",
        "# Definition: Accuracy is the most intuitive performance metric and represents the proportion of correctly classified samples out of the total number of samples.\n",
        "# Formula: Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)\n",
        "# Interpretation: In your case, accuracy would tell you the percentage of network connections that your model correctly classified as either normal or an intrusion (DoS, R2L, U2R, Probe). For example, an accuracy of 0.95 would mean that the model correctly classified 95% of the connections in the test dataset.\n",
        "# F1-Score:\n",
        "\n",
        "# Definition: The F1-score is a more comprehensive metric that considers both precision and recall. It is the harmonic mean of precision and recall, providing a balanced measure of a model's performance, especially when dealing with imbalanced datasets.\n",
        "# Precision: Precision measures the proportion of correctly predicted positive instances (intrusions) out of all instances predicted as positive. It answers the question: \"Of all the connections the model flagged as intrusions, how many were actually intrusions?\"\n",
        "# Recall: Recall measures the proportion of correctly predicted positive instances (intrusions) out of all actual positive instances. It answers the question: \"Of all the actual intrusions, how many did the model correctly identify?\"\n",
        "# Formula: F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "# Interpretation: In your intrusion detection scenario, the F1-score provides a balanced measure of how well the model identifies intrusions while minimizing false positives and false negatives. A higher F1-score indicates a better balance between precision and recall, which is desirable in security applications where both identifying true intrusions and minimizing false alarms are important.\n",
        "# Why both are important:\n",
        "\n",
        "# Accuracy alone can be misleading, especially with imbalanced datasets. For example, if your dataset has 95% normal connections and only 5% intrusions, a model that simply predicts \"normal\" for every connection would achieve 95% accuracy, even though it fails to detect any intrusions.\n",
        "# The F1-score provides a more balanced evaluation by considering both precision and recall, making it more suitable for intrusion detection where identifying true intrusions is critical, even if it means some false alarms.\n",
        "# In your code:\n",
        "\n",
        "\n",
        "# accuracy = accuracy_score(labels_test, predicted_labels)\n",
        "# fscore = f1_score(labels_test, predicted_labels)\n",
        "# Use code with caution\n",
        "# These lines calculate the accuracy and F1-score of your model based on the true labels (labels_test) and the predicted labels (predicted_labels) for the test dataset. These metrics help you evaluate the performance of your intrusion detection model.\n",
        "\n",
        "# I hope this clarifies the meaning and importance of accuracy_score and f1_score in your case. Let me know if you have any further questions.\n",
        "# To do...\n",
        "# Show the results: accuracy and training time\n",
        "print('Accuracy:', accuracy)\n",
        "print('F1-Score:', fscore)\n",
        "print('Taining time:', training_time)\n",
        "\n",
        "\n",
        "# Go back to Step 3 to adjust the hyper-parameters and retrain the model, to achieve better results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twHlkCu22jbJ"
      },
      "source": [
        "# Step 5:\n",
        "Write the Python code to:\n",
        "- Select the five most important features with a feature selection algorithm or provide a reasonable explanation.\n",
        "- Re-run the algorithm with the new datasets.\n",
        "- Recalculate the Accuracy and F1-Score and compare these metrics to your previous results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CixOV6S5Fjh",
        "outputId": "a80b711c-25d9-46de-cc74-ff81dbe8c17a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# ... (Previous code for data loading, preprocessing, and model training) ...\n",
        "\n",
        "# 1. Feature Selection:\n",
        "# Get feature importances from the trained model\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Get the indices of the top 5 features\n",
        "top_5_feature_indices = np.argsort(feature_importances)[-5:]\n",
        "\n",
        "# Get the names of the top 5 features (if you have feature names)\n",
        "# Assuming 'train_df' has the feature names as columns\n",
        "top_5_feature_names = train_df.columns[top_5_feature_indices]\n",
        "\n",
        "print(\"Top 5 important features:\", top_5_feature_names)\n",
        "\n",
        "# 2. Re-run with Selected Features:\n",
        "# Create new training and testing datasets with only the top 5 features\n",
        "features_train_selected = features_train[:, top_5_feature_indices]\n",
        "features_test_selected = features_test[:, top_5_feature_indices]\n",
        "\n",
        "# Re-train the model with selected features\n",
        "#model_selected = DecisionTreeClassifier()\n",
        "model_selected = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "model_selected.fit(features_train_selected, labels_train)\n",
        "\n",
        "# 3. Re-calculate Metrics and Compare:\n",
        "# Make predictions on the test set with the selected features\n",
        "predicted_labels_selected = model_selected.predict(features_test_selected)\n",
        "\n",
        "# Calculate Accuracy and F1-Score for the selected features model\n",
        "accuracy_selected = accuracy_score(labels_test, predicted_labels_selected)\n",
        "f1_selected = f1_score(labels_test, predicted_labels_selected, average='weighted')\n",
        "\n",
        "# Print and compare the results\n",
        "print(\"\\nOriginal Model:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1-Score:\", fscore)\n",
        "\n",
        "print(\"\\nSelected Features Model:\")\n",
        "print(\"Accuracy:\", accuracy_selected)\n",
        "print(\"F1-Score:\", f1_selected)\n",
        "\n",
        "# Compare the metrics and analyze the impact of feature selection"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
