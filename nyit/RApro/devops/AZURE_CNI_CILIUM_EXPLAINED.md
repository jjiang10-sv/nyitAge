# Azure CNI Overlay vs Cilium Overlay - The Truth

## Quick Answer

Your configuration uses: **Azure CNI Overlay with Cilium Dataplane** (HYBRID)

```python
network_profile=ContainerServiceNetworkProfileArgs(
    network_plugin="azure",           # â† Azure CNI (IPAM + overlay)
    network_plugin_mode="overlay",    # â† Azure manages overlay
    network_dataplane="cilium",       # â† Cilium handles packets (eBPF)
)
```

**This is NOT:**
- âŒ Pure Cilium overlay
- âŒ Pure Azure CNI overlay

**This IS:**
- âœ… **Azure CNI Overlay** (control plane + IPAM)
- âœ… **Cilium** (dataplane + eBPF packet processing)
- âœ… Best of both worlds!

---

## Understanding the Split

### Control Plane vs Dataplane

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONTROL PLANE (Who decides what)            â”‚
â”‚ - IP address allocation (IPAM)              â”‚
â”‚ - Pod CIDR management                       â”‚
â”‚ - Overlay network setup                     â”‚
â”‚ - Route distribution                        â”‚
â”‚                                             â”‚
â”‚ Owner: Azure CNI                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATAPLANE (Who moves packets)               â”‚
â”‚ - Packet forwarding                         â”‚
â”‚ - Load balancing (services)                 â”‚
â”‚ - Network policies                          â”‚
â”‚ - Encapsulation/decapsulation               â”‚
â”‚                                             â”‚
â”‚ Owner: Cilium (eBPF)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Your Configuration Breakdown

### Parameter 1: `network_plugin="azure"`

**What It Means:**
```
Uses Azure CNI as the CNI plugin
```

**Azure CNI Responsibilities:**
- Allocates IP addresses to pods
- Manages the pod CIDR (10.32.0.0/13)
- Sets up the overlay network infrastructure
- Integrates with Azure VNet
- Handles pod-to-pod routing metadata

**NOT Cilium CNI!**

---

### Parameter 2: `network_plugin_mode="overlay"`

**What It Means:**
```
Azure CNI operates in overlay mode
(not traditional mode where pods get VNet IPs)
```

**Azure CNI Overlay:**
- Creates a **separate pod network** (10.32.0.0/13)
- Pods get IPs from this overlay range
- Azure manages the encapsulation infrastructure
- Uses **Geneve** protocol by default

**This is the key**: Azure, not Cilium, sets up the overlay.

---

### Parameter 3: `network_dataplane="cilium"`

**What It Means:**
```
Replace default dataplane with Cilium's eBPF dataplane
```

**Cilium Dataplane Responsibilities:**
- **Packet forwarding** using eBPF (not iptables)
- **Service load balancing** using eBPF maps
- **Network policies** using eBPF programs
- **Observability** (Hubble)
- **Advanced features** (BGP, service mesh, etc.)

**Cilium does NOT:**
- Manage IP allocation (Azure CNI does this)
- Set up overlay infrastructure (Azure does this)
- Control the pod CIDR (Azure does this)

---

## The Three Modes of AKS Networking

### Mode 1: Azure CNI (Traditional)

```python
network_profile=ContainerServiceNetworkProfileArgs(
    network_plugin="azure",
    # No network_plugin_mode specified
    # No network_dataplane specified
)
```

**Architecture:**
```
Pods get IPs from VNet directly
Pod CIDR = VNet subnet CIDR
Dataplane = Linux kernel + iptables
```

**Pros:**
- âœ… Simple integration with VNet
- âœ… Direct routing

**Cons:**
- âŒ Consumes VNet IPs rapidly
- âŒ IP exhaustion risk
- âŒ iptables performance issues at scale

---

### Mode 2: Azure CNI Overlay (without Cilium)

```python
network_profile=ContainerServiceNetworkProfileArgs(
    network_plugin="azure",
    network_plugin_mode="overlay",
    # network_dataplane not specified = default Linux kernel
)
```

**Architecture:**
```
Control Plane: Azure CNI
- IPAM
- Overlay setup (Geneve)
- Pod CIDR management

Dataplane: Linux kernel
- iptables for services
- Linux routing
- Standard kernel networking
```

**Pros:**
- âœ… Separate pod IP space
- âœ… No VNet IP exhaustion

**Cons:**
- âŒ iptables performance limits
- âŒ No advanced observability
- âŒ Limited network policy features

---

### Mode 3: Azure CNI Overlay + Cilium Dataplane (YOUR SETUP)

```python
network_profile=ContainerServiceNetworkProfileArgs(
    network_plugin="azure",
    network_plugin_mode="overlay",
    network_dataplane="cilium",  # ğŸ¯ The difference!
)
```

**Architecture:**
```
Control Plane: Azure CNI
- IPAM (IP allocation)
- Overlay infrastructure (Geneve)
- Pod CIDR: 10.32.0.0/13
- Integration with Azure

Dataplane: Cilium
- eBPF packet processing
- eBPF service load balancing
- eBPF network policies
- Hubble observability
- Advanced features (BGP, mesh, etc.)
```

**Pros:**
- âœ… Separate pod IP space (no VNet exhaustion)
- âœ… eBPF performance (10x faster than iptables)
- âœ… Advanced network policies
- âœ… Hubble observability
- âœ… Azure integration
- âœ… Best of both worlds!

**Cons:**
- âš ï¸ Slightly more complex
- âš ï¸ Cilium learning curve

---

## Why This Hybrid Approach?

### What Microsoft Did

Microsoft wanted to leverage **Cilium's eBPF dataplane** without completely replacing Azure CNI:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure CNI                                 â”‚
â”‚ (What Microsoft knows well)               â”‚
â”‚ - Azure VNet integration                  â”‚
â”‚ - IPAM that works with Azure              â”‚
â”‚ - Overlay that integrates with SDN        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚                             â”‚
               â†“                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Option 1: Linux Kernel   â”‚  â”‚ Option 2: Cilium eBPF   â”‚
â”‚ - iptables               â”‚  â”‚ - eBPF programs         â”‚
â”‚ - Slow at scale          â”‚  â”‚ - 10x faster            â”‚
â”‚ - Limited features       â”‚  â”‚ - Advanced features     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â†‘
                              You chose this! âœ…
```

**Result:**
- Azure handles the **"Azure stuff"** (VNet, IPAM, overlay)
- Cilium handles the **"fast packet stuff"** (eBPF, policies, observability)

---

## Pure Cilium Overlay (For Comparison)

If you used **pure Cilium** (like in non-AKS Kubernetes):

```yaml
# Cilium ConfigMap (pure Cilium)
ipam:
  mode: kubernetes              # Cilium controls IP allocation
tunnel: geneve                 # Cilium manages overlay
datapath-mode: vxlan           # Cilium does encapsulation
```

**Differences:**
- Cilium manages **everything** (IPAM + dataplane)
- No Azure CNI involvement
- More Cilium-native features
- Less Azure integration

**In AKS, you can't do this!** Azure CNI is always the control plane.

---

## What Actually Happens in Your Setup

### Pod Creation Flow

```
1. Pod is scheduled to Node
   â†“
2. Azure CNI Plugin Called
   â”œâ”€ Allocates IP from pod CIDR (10.32.x.x)
   â”œâ”€ Sets up overlay interface
   â”œâ”€ Configures routes
   â””â”€ Tells Cilium about the pod
   â†“
3. Cilium Takes Over
   â”œâ”€ Installs eBPF programs
   â”œâ”€ Sets up efficient packet handling
   â”œâ”€ Configures service load balancing
   â””â”€ Enables Hubble monitoring
```

### Packet Flow (Pod to Pod)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pod A (10.32.1.10) sends packet         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cilium eBPF Program (on Node A)         â”‚
â”‚ - Looks up destination                  â”‚
â”‚ - Finds Pod B on Node B                 â”‚
â”‚ - Decides to encapsulate                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure CNI Overlay Infrastructure        â”‚
â”‚ - Uses Geneve encapsulation             â”‚
â”‚ - Outer: Node A IP â†’ Node B IP          â”‚
â”‚ - Inner: Pod A IP â†’ Pod B IP            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure VNet Routes Packet                â”‚
â”‚ - Sees Node A â†’ Node B                  â”‚
â”‚ - Routes via VNet                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cilium eBPF Program (on Node B)         â”‚
â”‚ - Decapsulates packet                   â”‚
â”‚ - Delivers to Pod B                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Point:** 
- **Encapsulation format**: Azure CNI's Geneve
- **Packet processing**: Cilium's eBPF

---

## Feature Ownership Table

| Feature | Azure CNI | Cilium | Who Wins? |
|---------|-----------|--------|-----------|
| **IP Address Allocation** | âœ… | âŒ | Azure CNI |
| **Pod CIDR Management** | âœ… | âŒ | Azure CNI |
| **Overlay Setup** | âœ… | âŒ | Azure CNI |
| **Encapsulation Protocol** | âœ… (Geneve) | âŒ | Azure CNI |
| **Packet Forwarding** | âŒ | âœ… (eBPF) | Cilium |
| **Service Load Balancing** | âŒ | âœ… (eBPF) | Cilium |
| **Network Policies** | âŒ | âœ… (eBPF) | Cilium |
| **Observability (Hubble)** | âŒ | âœ… | Cilium |
| **BGP Support** | âŒ | âœ… | Cilium |
| **Service Mesh** | âŒ | âœ… | Cilium |

---

## Verification Commands

### Check Who's Managing What

```bash
# Check CNI plugin
kubectl get pods -n kube-system -o wide | grep azure-cni
# Should see azure-cni-* pods

# Check Cilium dataplane
kubectl get pods -n kube-system -o wide | grep cilium
# Should see cilium-* pods

# Check Cilium status
kubectl -n kube-system exec ds/cilium -- cilium status

# Output shows:
# KubeProxyReplacement: Strict      â† Cilium handles services
# Cilium:               OK
# IPAM:                 Azure        â† Azure handles IP allocation! ğŸ¯
```

### Check IPAM Mode

```bash
kubectl -n kube-system exec ds/cilium -- cilium status | grep -i ipam

# Expected output:
# IPAM: Azure                    â† Azure CNI manages IPs
# (NOT "IPAM: Cluster Pool" which would be pure Cilium)
```

### Check Overlay

```bash
kubectl -n kube-system exec ds/cilium -- cilium status | grep -i tunnel

# Expected output:
# Encapsulation: Geneve          â† Azure CNI's overlay
# (Cilium is aware but not managing it)
```

---

## Common Misconceptions

### âŒ Misconception 1: "Using Cilium = Pure Cilium"

**Reality:**
```
In AKS, Cilium is the DATAPLANE only.
Azure CNI is still the control plane.
```

### âŒ Misconception 2: "Azure CNI Overlay doesn't use Cilium"

**Reality:**
```
You can choose:
- Azure CNI Overlay + Linux kernel (default)
- Azure CNI Overlay + Cilium dataplane (our choice âœ…)
```

### âŒ Misconception 3: "Cilium manages the overlay"

**Reality:**
```
Azure CNI sets up the overlay (Geneve).
Cilium uses it but doesn't control it.
```

---

## Why This Matters

### 1. Troubleshooting

**IP Address Issues:**
```bash
# Check Azure CNI logs (IPAM problems)
kubectl logs -n kube-system -l component=azure-cni

# Check Cilium logs (packet forwarding problems)
kubectl logs -n kube-system -l k8s-app=cilium
```

### 2. Configuration

**Pod CIDR changes:**
```python
# This is Azure CNI configuration
pod_cidr="10.32.0.0/13"  # â† Azure CNI uses this
```

**Cilium features:**
```yaml
# Cilium Helm values
hubble:
  enabled: true  # â† Cilium feature, works fine
```

### 3. Limitations

**Can't do (Azure CNI limitations):**
- Custom IPAM modes
- Pure Cilium cluster mesh (need workarounds)
- Direct routing without overlay (Azure decides)

**Can do (Cilium features):**
- eBPF-based network policies
- Hubble observability
- L7 traffic management
- BGP (with limitations)

---

## Benefits of This Hybrid

### Why Microsoft Chose This

1. **Azure Integration** âœ…
   - Works with Azure VNet
   - Compatible with Azure Firewall
   - Integrates with Azure Policy

2. **Performance** âœ…
   - eBPF is 10x faster than iptables
   - Better service load balancing
   - Lower CPU usage

3. **Observability** âœ…
   - Hubble for network visibility
   - Better than basic Azure monitoring

4. **Stability** âœ…
   - Azure CNI is battle-tested in AKS
   - Gradual Cilium adoption = less risk

5. **Flexibility** âœ…
   - Can swap dataplane (Cilium â†” default)
   - Azure CNI stays stable

---

## Summary

### Your Configuration:

```python
network_plugin="azure"           # Azure CNI is the boss
network_plugin_mode="overlay"    # Azure CNI creates overlay
network_dataplane="cilium"       # Cilium processes packets
```

### Answer: **Azure CNI Overlay** (with Cilium Dataplane)

**Division of Labor:**

| Responsibility | Owner |
|----------------|-------|
| IP Allocation | Azure CNI |
| Overlay Setup | Azure CNI |
| Encapsulation | Azure CNI (Geneve) |
| VNet Integration | Azure CNI |
| Packet Forwarding | **Cilium eBPF** |
| Service LB | **Cilium eBPF** |
| Network Policies | **Cilium eBPF** |
| Observability | **Cilium Hubble** |

**Think of it as:**
- Azure CNI = The architect (designs the network)
- Cilium = The builder (moves the packets efficiently)

---

## Quick Test

```bash
# Verify it's hybrid mode
kubectl -n kube-system exec ds/cilium -- cilium status

# Key indicators:
# âœ… IPAM: Azure              (Azure CNI manages IPs)
# âœ… Encapsulation: Geneve    (Azure CNI's overlay)
# âœ… KubeProxyReplacement: Strict  (Cilium handles services)
# âœ… Cilium: OK               (Cilium dataplane active)
```

**Conclusion:** You're using **Azure CNI Overlay** for the control plane and **Cilium eBPF** for the dataplane - the best of both worlds! ğŸ¯
