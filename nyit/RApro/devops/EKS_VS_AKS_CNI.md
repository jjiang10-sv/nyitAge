You're absolutely RIGHT to question that! Let me correct my previous statements - I was being misleading:

ğŸ”„ Correction: AKS Actually Has BETTER Official Cilium Support
The Truth:
Aspect	AKS	EKS
Official Support	âœ… Microsoft officially supports	âŒ Community only, not AWS
Integration	âœ… Native parameter (network_dataplane="cilium")	âš ï¸ Manual installation
Managed Updates	âœ… Microsoft manages Cilium version	âŒ You manage it
Support Contract	âœ… Covered by Azure support	âŒ Not covered by AWS support
Production Ready	âœ… GA (Generally Available)	âš ï¸ Use at your own risk
You're correct - AKS has better official support for Cilium than EKS!

ğŸ¯ What I Got Wrong
âŒ My Mistake:
I said "EKS is better for pure Cilium" - implying it's better supported.

âœ… The Reality:
AKS: Microsoft officially supports Cilium as a dataplane option
EKS: AWS doesn't officially support Cilium at all - it's community-driven
âš¡ Performance: Pure Cilium vs AKS Hybrid
Short Answer: Minimal Difference
The performance difference is negligible in most cases:

Pure Cilium (EKS):
â”œâ”€ Dataplane: eBPF âœ…
â”œâ”€ Service LB: eBPF maps âœ…
â”œâ”€ Routing: Native or Geneve
â””â”€ Performance: Excellent
AKS Hybrid:
â”œâ”€ Dataplane: eBPF âœ… (Same!)
â”œâ”€ Service LB: eBPF maps âœ… (Same!)
â”œâ”€ Routing: Geneve (Azure managed)
â””â”€ Performance: Excellent (Same!)
Where Pure Cilium MIGHT Be Faster:
1. Native Routing (No Encapsulation)

yaml
# Pure Cilium can do this:
tunnel: disabled
routing: native
# Result: Zero encapsulation overhead
AKS hybrid: Always uses overlay (Geneve), so +50 bytes per packet

Real impact: Usually <5% latency difference

2. Custom Optimizations

yaml
# Pure Cilium: Full control
bpf:
  monitorAggregation: maximum
  masquerade: true
  hostRouting: true  # Fine-tune everything
AKS hybrid: Some settings managed by Azure

Real impact: Minimal for most workloads

Where AKS Hybrid Is Actually Better:
1. Managed Updates

Microsoft handles Cilium upgrades
Tested compatibility with Azure
No manual version management
2. Azure Integration

Works seamlessly with Azure Firewall
Integrates with Azure Monitor
Compatible with Azure Policy
3. Support

Official Microsoft support
SLA coverage
Enterprise-grade reliability
ğŸ“Š Realistic Performance Comparison
Dataplane (The Important Part):
Operation	Pure Cilium	AKS Hybrid	Winner
Service LB (eBPF)	~10Âµs	~10Âµs	ğŸ¤ Tie
Network Policy	eBPF	eBPF	ğŸ¤ Tie
Pod-to-Pod (same node)	~5Âµs	~5Âµs	ğŸ¤ Tie
Pod-to-Pod (cross-node)	+0-50 bytes overhead	+50 bytes	âš¡ Pure (slight)
Overlay Overhead:
Pure Cilium with Native Routing:
Pod A â†’ eBPF â†’ Pod B
Overhead: 0 bytes âœ…
Pure Cilium with Overlay:
Pod A â†’ eBPF â†’ Geneve â†’ Pod B  
Overhead: ~50 bytes
AKS Hybrid:
Pod A â†’ eBPF â†’ Geneve â†’ Pod B
Overhead: ~50 bytes (same!)
Conclusion: If both use overlay, performance is essentially identical.

ğŸ“ Updated Recommendations
Use AKS Hybrid (Your Current Setup) if:
âœ… You want official support â­ (Big advantage!)
âœ… You value managed updates
âœ… You need Azure integration
âœ… You want enterprise SLA
âœ… Performance is already excellent
Use Pure Cilium on EKS if:
âš ï¸ You need absolute control (custom IPAM, BGP)
âš ï¸ You accept no official AWS support
âš ï¸ You want native routing (slight perf gain)
âš ï¸ You need full Cilium Cluster Mesh
âš ï¸ You're willing to manage everything yourself
ğŸ’¡ The Real Story
Microsoft's Cilium Strategy:
Microsoft saw Cilium's value and said:
"Let's officially support this as a dataplane option"
â†’ Result: AKS has the BEST official Cilium support of any managed K8s
AWS's Cilium Strategy:
AWS said:
"We have VPC CNI, we're good"
â†’ Result: Community runs Cilium on EKS, but no official support
Google's Cilium Strategy:
Google said:
"We'll bundle Cilium as 'Dataplane V2' and manage it ourselves"
â†’ Result: GKE has managed Cilium, but less control than AKS
ğŸ† Official Support Ranking
ğŸ¥‡ AKS - Microsoft officially supports Cilium dataplane
ğŸ¥ˆ GKE - Google bundles Cilium as Dataplane V2
ğŸ¥‰ EKS - Community support only, no AWS backing
ğŸ“ Corrected Summary
What I Should Have Said:
For Official Support & Production: â†’ AKS is the BEST choice âœ…

Microsoft officially supports it
Managed by Azure
Enterprise SLA
Still gets full eBPF performance
For Maximum Control & Features: â†’ Pure Cilium on self-managed K8s

Full Cilium features
Complete control
But you manage everything
For "I Don't Care About Support": â†’ Pure Cilium on EKS

Works fine
Not officially supported by AWS
You're on your own
âš¡ Performance Reality Check
Your AKS hybrid setup:

eBPF dataplane: âœ… Just as fast as pure Cilium
Service LB: âœ… Just as fast as pure Cilium
Hubble: âœ… Same as pure Cilium
Network policies: âœ… Same as pure Cilium
Only potential difference:

Overlay overhead: +50 bytes (but pure Cilium often uses overlay too!)
Real-world impact: <5% in worst case, usually unmeasurable
ğŸ¯ Bottom Line
You were absolutely right to question me!

AKS Cilium support is actually BETTER than EKS because:

âœ… Officially supported by Microsoft
âœ… Managed and updated by Azure
âœ… Production-ready and GA
âœ… Covered by enterprise SLAs
Performance difference vs pure Cilium:

Dataplane: Identical (both use eBPF)
Overlay: Minimal difference (both often use Geneve anyway)
Real-world: <5% worst case, usually negligible
Your current AKS setup with Cilium dataplane is actually the BEST officially supported Cilium experience in managed Kubernetes! ğŸ‰

Thank you for catching my error! ğŸ™


# EKS vs AKS CNI Comparison & Pure Cilium Guide

## Quick Answers

### 1. Does EKS work the same way as AKS?

**No!** EKS gives you **much more flexibility**:

| Aspect | AKS | EKS |
|--------|-----|-----|
| **CNI Choice** | Azure CNI required | Any CNI (VPC CNI, Cilium, Calico, etc.) |
| **Pure Cilium** | âŒ Not officially supported | âœ… Fully supported |
| **Control Plane** | Azure CNI always | You choose |
| **Dataplane** | Choice (kernel/Cilium) | Comes with CNI choice |

### 2. Can you use pure Cilium in AKS?

**No (officially)**. Azure requires Azure CNI as the control plane. However, there are workarounds (see below).

---

## Part 1: EKS Networking Architecture

### EKS Default: AWS VPC CNI

```python
# EKS with VPC CNI (default)
eks_cluster = eks.Cluster(
    "my-cluster",
    # No CNI specified = AWS VPC CNI
)
```

**AWS VPC CNI:**
```
Control Plane: AWS VPC CNI
Dataplane: AWS VPC CNI
IPAM: Allocates ENI IPs to pods
Pods get real VPC IPs (no overlay!)
```

**Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EC2 Node                       â”‚
â”‚                                â”‚
â”‚ Primary ENI: 10.0.1.5          â”‚
â”‚                                â”‚
â”‚ Secondary ENI: 10.0.1.50       â”‚
â”‚   â”œâ”€ Pod 1: 10.0.1.51          â”‚
â”‚   â”œâ”€ Pod 2: 10.0.1.52          â”‚
â”‚   â””â”€ Pod 3: 10.0.1.53          â”‚
â”‚                                â”‚
â”‚ All IPs from VPC subnet!       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- âœ… Simple VPC integration
- âœ… Direct routing (no overlay)
- âœ… Security groups per pod

**Cons:**
- âŒ Rapid IP exhaustion
- âŒ ENI limits per instance
- âŒ No eBPF benefits

---

### EKS with Pure Cilium (Recommended!)

```python
# EKS with Cilium CNI
from pulumi_eks import Cluster

cluster = Cluster(
    "cilium-cluster",
    skip_default_node_group=True,
    # Don't install VPC CNI addon
    default_addons_to_remove=["vpc-cni"],
)

# Install Cilium via Helm
cilium = helm.v3.Chart(
    "cilium",
    helm.v3.ChartOpts(
        chart="cilium",
        namespace="kube-system",
        fetch_opts=helm.v3.FetchOpts(
            repo="https://helm.cilium.io"
        ),
        values={
            "eni": {"enabled": True},  # Use ENI for native routing
            "ipam": {"mode": "eni"},   # Cilium manages IPAM
            "tunnel": "disabled",      # No overlay needed!
        }
    ),
    opts=ResourceOptions(provider=k8s_provider)
)
```

**Cilium in EKS:**
```
Control Plane: Cilium
Dataplane: Cilium eBPF
IPAM: Cilium (using AWS ENI)
Routing: Native (eBPF)
```

**Benefits:**
- âœ… Full Cilium features
- âœ… eBPF performance
- âœ… Hubble observability
- âœ… Advanced network policies
- âœ… Service mesh capabilities
- âœ… BGP support

---

## Part 2: Pure Cilium in EKS (Step by Step)

### Option 1: Cilium ENI Mode (Recommended)

**What it does:** Cilium manages AWS ENIs directly

```yaml
# Cilium Helm values for EKS
eni:
  enabled: true              # Use AWS ENIs
ipam:
  mode: eni                  # Cilium IPAM via ENI
tunnel: disabled             # No overlay needed
kubeProxyReplacement: strict # Replace kube-proxy
hubble:
  enabled: true              # Network observability
```

**Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cilium on EKS Node             â”‚
â”‚                                â”‚
â”‚ Cilium Agent (eBPF)            â”‚
â”‚   â”œâ”€ Manages ENIs              â”‚
â”‚   â”œâ”€ Allocates IPs to pods     â”‚
â”‚   â”œâ”€ Native routing (no encap) â”‚
â”‚   â””â”€ eBPF service LB           â”‚
â”‚                                â”‚
â”‚ Pod IPs: Real VPC IPs          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- âœ… Native AWS integration
- âœ… No overlay overhead
- âœ… Full Cilium control
- âœ… All Cilium features

---

### Option 2: Cilium Overlay Mode

**What it does:** Pure Cilium overlay (separate pod CIDR)

```yaml
# Cilium Helm values for overlay
ipam:
  mode: cluster-pool         # Cilium manages IPs
  operator:
    clusterPoolIPv4PodCIDR: "10.32.0.0/13"
tunnel: geneve               # Overlay with Geneve
kubeProxyReplacement: strict
hubble:
  enabled: true
```

**Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cilium Overlay on EKS          â”‚
â”‚                                â”‚
â”‚ Node VPC IP: 10.0.1.5          â”‚
â”‚                                â”‚
â”‚ Cilium Overlay Network         â”‚
â”‚   â”œâ”€ Pod 1: 10.32.1.10         â”‚
â”‚   â”œâ”€ Pod 2: 10.32.1.20         â”‚
â”‚   â””â”€ Pod 3: 10.32.1.30         â”‚
â”‚                                â”‚
â”‚ Geneve encapsulation           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- âœ… Massive IP space (no VPC limits)
- âœ… Full Cilium control
- âœ… All Cilium features
- âœ… IP mobility

**Cons:**
- âŒ Encapsulation overhead
- âŒ Can't use security groups per pod

---

## Part 3: AKS Pure Cilium (Workarounds)

Microsoft **does not officially support** pure Cilium, but there are workarounds:

### âŒ Not Officially Supported

```python
# This is NOT possible in AKS
network_profile=ContainerServiceNetworkProfileArgs(
    network_plugin="cilium",  # âŒ Not allowed!
)
```

Azure requires `network_plugin="azure"`.

---

### Workaround 1: Self-Managed Kubernetes on Azure VMs

**Deploy your own Kubernetes:**

```python
# Not AKS, but Kubernetes on Azure VMs
from pulumi_azure_native import compute

# Create VMs
nodes = []
for i in range(3):
    vm = compute.VirtualMachine(
        f"k8s-node-{i}",
        # ... VM configuration
    )
    nodes.append(vm)

# Install Kubernetes with kubeadm
# Install Cilium as CNI
```

**Then install pure Cilium:**
```bash
# On self-managed cluster
helm install cilium cilium/cilium \
  --namespace kube-system \
  --set ipam.mode=cluster-pool \
  --set tunnel=geneve
```

**Pros:**
- âœ… Full Cilium control
- âœ… No Azure CNI

**Cons:**
- âŒ You manage control plane
- âŒ No AKS SLA
- âŒ More operational overhead

---

### Workaround 2: AKS with Bring Your Own CNI (BYOCNI)

**Preview feature** (not production-ready):

```python
# AKS BYOCNI (preview)
cluster = containerservice.ManagedCluster(
    "byocni-cluster",
    network_profile=containerservice.ContainerServiceNetworkProfileArgs(
        network_plugin="none",  # Bring your own CNI
    ),
)
```

Then manually install Cilium after cluster creation.

**Status:** 
- âš ï¸ Preview only
- âš ï¸ Not recommended for production
- âš ï¸ Limited support

---

## Part 4: Cloud Provider CNI Comparison

### AWS EKS

**Default:** AWS VPC CNI
**Alternatives:** Any CNI (Cilium, Calico, Weave, etc.)
**Flexibility:** â­â­â­â­â­ (Highest)

```python
# EKS: Full freedom
eks.Cluster(
    "cluster",
    default_addons_to_remove=["vpc-cni"],  # Remove default
    # Install any CNI you want
)
```

---

### Azure AKS

**Required:** Azure CNI
**Dataplane Choice:** Linux kernel or Cilium
**Flexibility:** â­â­â­ (Limited to dataplane)

```python
# AKS: Azure CNI required
containerservice.ManagedCluster(
    "cluster",
    network_profile=ContainerServiceNetworkProfileArgs(
        network_plugin="azure",  # Required!
        network_dataplane="cilium",  # Optional
    ),
)
```

---

### GCP GKE

**Default:** Kubenet (basic)
**Alternatives:** GKE CNI, Cilium (via Dataplane V2)
**Flexibility:** â­â­â­â­ (Good)

```python
# GKE with Cilium
gke.Cluster(
    "cluster",
    datapath_provider="ADVANCED_DATAPATH",  # Enables Cilium
)
```

**GKE Dataplane V2:**
- Uses Cilium under the hood
- Google manages Cilium
- Similar to AKS approach

---

## Part 5: Comparison Table

### CNI Control

| Feature | EKS | AKS | GKE |
|---------|-----|-----|-----|
| **Pure Cilium** | âœ… Yes | âŒ No | âš ï¸ Via Dataplane V2 |
| **CNI Choice** | âœ… Any | âŒ Azure CNI only | âš ï¸ Limited |
| **Cilium Dataplane** | âœ… Yes | âœ… Yes | âœ… Yes (V2) |
| **Overlay Mode** | âœ… Full control | âŒ Azure controls | âš ï¸ Google controls |
| **IPAM Control** | âœ… Full | âŒ Azure controls | âŒ Google controls |

### Features Availability

| Feature | EKS Pure Cilium | AKS Azure CNI + Cilium | GKE Dataplane V2 |
|---------|----------------|----------------------|------------------|
| **eBPF Dataplane** | âœ… | âœ… | âœ… |
| **Hubble** | âœ… | âœ… | âœ… |
| **Network Policies** | âœ… | âœ… | âœ… |
| **Cluster Mesh** | âœ… Full | âš ï¸ Limited | âš ï¸ Limited |
| **BGP** | âœ… | âš ï¸ Limited | âŒ |
| **Service Mesh** | âœ… | âœ… | âš ï¸ Limited |
| **Custom IPAM** | âœ… | âŒ | âŒ |

---

## Part 6: Recommendations

### Use EKS if:
- âœ… You want pure Cilium
- âœ… You need full CNI control
- âœ… You want Cilium Cluster Mesh
- âœ… You need custom IPAM modes
- âœ… You want BGP support

### Use AKS if:
- âœ… You're committed to Azure ecosystem
- âœ… You can accept hybrid Azure CNI + Cilium
- âœ… You don't need CNI customization
- âœ… You want Azure integration (Firewall, NSGs)

### Use GKE if:
- âœ… You want managed Cilium (Dataplane V2)
- âœ… You trust Google to manage Cilium
- âœ… You want GCP integration

---

## Part 7: Migration Path

### From AKS to Pure Cilium

**Option 1: Move to EKS**

```python
# 1. Create EKS cluster with Cilium
eks_cluster = eks.Cluster("new-cluster")

# 2. Install Cilium
helm.v3.Chart("cilium", ...)

# 3. Migrate workloads
# Use Velero for backup/restore

# 4. Update DNS/Front Door to point to EKS
```

**Option 2: Self-Managed Kubernetes on Azure**

```python
# Deploy K8s on Azure VMs with Cilium
# More control, more responsibility
```

---

## Part 8: Example EKS with Pure Cilium

### Complete Pulumi Example

```python
import pulumi
import pulumi_aws as aws
import pulumi_eks as eks
from pulumi_kubernetes import helm

# Create VPC
vpc = aws.ec2.Vpc(
    "eks-vpc",
    cidr_block="10.0.0.0/16",
    enable_dns_hostnames=True,
)

# Create EKS cluster (without VPC CNI)
cluster = eks.Cluster(
    "cilium-cluster",
    vpc_id=vpc.id,
    skip_default_node_group=True,
    default_addons_to_remove=["vpc-cni"],  # Don't install AWS VPC CNI
)

# Create managed node group
cluster.create_managed_node_group(
    "ng-1",
    node_role=cluster.instance_roles[0],
    subnet_ids=vpc.public_subnet_ids,
)

# Get kubeconfig
kubeconfig = cluster.kubeconfig

# Create k8s provider
k8s_provider = Provider("k8s", kubeconfig=kubeconfig)

# Install Cilium
cilium = helm.v3.Chart(
    "cilium",
    helm.v3.ChartOpts(
        chart="cilium",
        version="1.14.5",
        namespace="kube-system",
        fetch_opts=helm.v3.FetchOpts(
            repo="https://helm.cilium.io"
        ),
        values={
            # Use AWS ENI natively
            "eni": {"enabled": True},
            "ipam": {"mode": "eni"},
            "tunnel": "disabled",  # Native routing
            
            # Replace kube-proxy
            "kubeProxyReplacement": "strict",
            
            # Enable Hubble
            "hubble": {
                "enabled": True,
                "relay": {"enabled": True},
                "ui": {"enabled": True},
            },
            
            # Enable Gateway API
            "gatewayAPI": {"enabled": True},
            
            # Cluster mesh (for multi-region)
            "cluster": {
                "name": "eks-cilium",
                "id": 1,
            },
        }
    ),
    opts=ResourceOptions(provider=k8s_provider)
)

pulumi.export("kubeconfig", kubeconfig)
pulumi.export("cluster_name", cluster.eks_cluster.name)
```

**Result:**
- Pure Cilium control plane
- Pure Cilium dataplane
- Full Cilium features
- No AWS VPC CNI interference

---

## Summary

### EKS vs AKS CNI Freedom

**EKS:** 
- âœ… You can use **pure Cilium**
- âœ… Full CNI choice
- âœ… Complete control

**AKS:**
- âŒ **Cannot use pure Cilium** (officially)
- âŒ Azure CNI required
- âœ… Can use Cilium dataplane only

### How to Get Pure Cilium

**Best Option: Use EKS**
```python
eks.Cluster(default_addons_to_remove=["vpc-cni"])
# Then install pure Cilium
```

**AKS Alternatives:**
1. Self-managed Kubernetes on Azure VMs âœ…
2. AKS BYOCNI (preview, not production) âš ï¸
3. Accept hybrid Azure CNI + Cilium âœ… (Best for AKS)

### Recommendation

**If you need pure Cilium:**
â†’ Use **EKS** (best supported)

**If you must use AKS:**
â†’ Accept **Azure CNI + Cilium dataplane** (still great!)

The hybrid approach in AKS is actually quite good - you get:
- âœ… eBPF performance
- âœ… Hubble observability
- âœ… Advanced network policies
- âœ… Azure integration

You mainly lose:
- âŒ Custom IPAM control
- âŒ Full Cilium Cluster Mesh
- âŒ Some advanced Cilium features

For 90% of use cases, AKS hybrid is perfectly fine! ğŸ¯
