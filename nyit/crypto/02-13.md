left encryption     right encryption

Diffusion    p-c
Confusion    k-c

transformation  : swap the left and right encryption -> make it hard to predict by compare p and c
substition:  exo  with the key -> make it hard to predict by compare k and c
sub

In abstract algeba

Groups
Abelian Groups
Rings
Commutative Rings
Integral Domains
Fields  Finite fields

elements    operations      groups
Rules : 


Project Report: K8s Autoscaling Project
This report summarizes the milestones, initial plan, and changes based on the project progress documented in the provided Excel file. The project focuses on implementing autoscaling in Kubernetes (K8s) using KEDA (Kubernetes Event-Driven Autoscaling) and integrating it with PredictKube for intelligent scaling decisions.

1. Initial Plan
The initial plan was to:

Set up a local Kubernetes cluster using MicroK8s.

Deploy KEDA for event-driven autoscaling.

Use Prometheus and Grafana for monitoring and observability.

Develop a sample web service (initially using Nginx) for load testing.

Use Vegeta for load testing and observe the scaling behavior.

Integrate PredictKube as a trigger for KEDA to enable intelligent scaling based on predictive metrics.

Test and debug the system to ensure smooth scaling under different load conditions.

2. Milestones
Week 1-3: Setup and Initial Testing
Week 1:

Set up MicroK8s with 2 CPUs and 4GB memory.

Deployed KEDA, Prometheus, and Grafana.

Created an Nginx service for testing and exposed it.

Conducted initial load testing using Vegeta.

Created a KEDA ScaledObject with Prometheus as a trigger.

Discussed the plan with the PredictKube team and Professor Amin.

Week 2:

Replaced Nginx with a custom Golang service (using the Gin framework) with a 5-minute startup time.

Conducted load testing and observed scaling behavior.

Started integrating PredictKube but faced issues with the API key.

Week 3:

Debugged PredictKube key invalidation issues.

Researched Vegeta load testing tool and integrated it with Prometheus.

Faced stability issues with the local K8s cluster under high load (above 1000 RPS).

Week 4-6: Migration to AWS EKS and Advanced Testing
Week 4:

Migrated the K8s cluster to AWS EKS for better stability.

Deployed KEDA and PredictKube in AWS EKS.

Researched integrating Vegeta metrics with Prometheus and Grafana.

Debugged service crashes in EKS and replaced the victim service.

Week 5:

Set up Prometheus metrics and Grafana dashboards to monitor HTTP requests.

Used Kong Ingress to expose the victim service for better stability during load testing.

Confirmed that PredictKube could be used for free in the project setup.

Week 6:

Presented AWS EKS and PredictKube options to Professor Amin.

Set up HTTP request metrics in Prometheus and Grafana.

Explored nested metrics to distinguish legitimate and DDoS traffic.

Tested scaling behavior with PredictKube settings.

Week 7-9: Debugging and Advanced Integration
Week 7:

Implemented nested thresholds to distinguish legitimate and DDoS requests.

Debugged PredictKube gRPC connection failures.

Switched to Prometheus as a trigger for scaling and observed successful scaling.

Updated Prometheus queries to prevent scaling on 3xx status codes.

Week 8:

Researched KEDA scalers, including PredictKube and Prometheus.

Used Telepresence to debug PredictKube gRPC connection failures.

Traced KEDA source code to understand scaling behavior.

Observed that KEDA's ScaledObject controller queries Prometheus API for metrics.

Week 9:

Experimented with running ScaledObject with multiple triggers (PredictKube and Prometheus).

Proxied Prometheus server locally to debug PredictKube failures.

Compared PredictKube and Prometheus triggers and identified PredictKube's limitations.

Week 10-11: Final Testing and Future Plans
Week 10:

Debugged KEDA operator and tested rate limiting.

Successfully scaled the target deployment using PredictKube trigger.

Continued researching KEDA and K8s libraries for improvements.

Week 11:

Planned to create a separate AWS account for testing.

Considered subscribing to PredictKube's paid service for better support.

Aimed to integrate a custom predictive model with KEDA and compare it with PredictKube.

3. Changes from Initial Plan
Migration to AWS EKS:

The local K8s cluster was unstable under high load, so the project migrated to AWS EKS for better performance and stability.

Replacement of Nginx with Custom Golang Service:

The initial Nginx service was replaced with a custom Golang service to simulate a 5-minute startup time and better control over behavior.

Integration of Kong Ingress:

Initially, port forwarding was used to expose services, but it was unstable under high load. Kong Ingress was introduced for better stability.

Switching from PredictKube to Prometheus Trigger:

Due to persistent issues with PredictKube's gRPC connection, the project temporarily switched to using Prometheus as the primary trigger for scaling.

Exploration of Nested Metrics:

The project explored using nested metrics to distinguish between legitimate and DDoS traffic, which was not part of the initial plan.

Debugging and Code Exploration:

Significant time was spent debugging KEDA and PredictKube, including tracing source code and experimenting with multiple triggers.

Future Plans for Paid PredictKube Service:

The team considered subscribing to PredictKube's paid service for better support and features, which was not part of the initial plan.

4. Key Learnings and Challenges
Key Learnings:
KEDA's Flexibility: KEDA supports multiple triggers (e.g., Prometheus, PredictKube) and can be customized for different scaling scenarios.

Importance of Observability: Prometheus and Grafana were critical for monitoring and debugging scaling behavior.

Cloud Migration: Migrating to AWS EKS improved stability and performance under high load.

Challenges:
PredictKube Integration: Persistent issues with PredictKube's gRPC connection delayed progress.

Local Cluster Limitations: The local K8s cluster was unstable under high load, necessitating a move to AWS EKS.

Complexity of Nested Metrics: Implementing nested metrics to distinguish traffic types was more challenging than anticipated.

5. Conclusion
The project successfully implemented autoscaling in Kubernetes using KEDA and integrated it with Prometheus for monitoring. While PredictKube integration faced challenges, the team explored alternative solutions and gained valuable insights into KEDA's scaling mechanisms. Future work includes resolving PredictKube issues, integrating a custom predictive model, and comparing it with PredictKube's performance.

This report highlights the project's progress, challenges, and adaptations, providing a clear overview of the milestones and changes from the initial plan.