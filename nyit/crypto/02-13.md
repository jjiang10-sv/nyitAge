
An **in-order traversal** of a binary tree is a method of visiting all the nodes in the tree in a specific order: **left subtree, root, right subtree**. This traversal is particularly useful for binary search trees (BSTs) because it visits the nodes in **ascending order** of their values.

### Binary Recursion
Binary recursion is a form of recursion where a function makes **two recursive calls**—one for the left subtree and one for the right subtree. In the case of in-order traversal, the algorithm follows this pattern:

1. **Recursively traverse the left subtree**.
2. **Visit the root node**.
3. **Recursively traverse the right subtree**.

### In-Order Traversal Algorithm
Here’s the step-by-step process for an in-order traversal using binary recursion:

1. **Base Case**: If the current node is `nil`, return (i.e., stop recursion).
2. **Recursive Case**:
   - Traverse the left subtree.
   - Visit the current node (e.g., print its value).
   - Traverse the right subtree.

### Example Walkthrough
Consider the following binary tree:

```
      4
     / \
    2   6
   / \ / \
  1  3 5  7
```

The in-order traversal of this tree will visit the nodes in the order: **1, 2, 3, 4, 5, 6, 7**.

### Go Implementation
Here’s a Go implementation of in-order traversal using binary recursion:

```go
type TreeNode struct {
    Val   int
    Left  *TreeNode
    Right *TreeNode
}

func InOrderTraversal(root *TreeNode) {
    // Base case: if the node is nil, return
    if root == nil {
        return
    }

    // Recursively traverse the left subtree
    InOrderTraversal(root.Left)

    // Visit the current node (print its value)
    fmt.Println(root.Val)

    // Recursively traverse the right subtree
    InOrderTraversal(root.Right)
}
```

### Explanation of the Code
1. **Base Case**: If the current node is `nil`, the function returns, stopping the recursion.
2. **Recursive Case**:
   - `InOrderTraversal(root.Left)`: Recursively traverse the left subtree.
   - `fmt.Println(root.Val)`: Visit the current node (e.g., print its value).
   - `InOrderTraversal(root.Right)`: Recursively traverse the right subtree.

### Walkthrough of the Example Tree
Let’s walk through the in-order traversal of the example tree:

1. Start at the root node (4).
   - Traverse the left subtree (2).
     - Traverse the left subtree (1).
       - Node 1 has no left child, so visit node 1.
     - Visit node 2.
     - Traverse the right subtree (3).
       - Node 3 has no left child, so visit node 3.
   - Visit node 4.
   - Traverse the right subtree (6).
     - Traverse the left subtree (5).
       - Node 5 has no left child, so visit node 5.
     - Visit node 6.
     - Traverse the right subtree (7).
       - Node 7 has no left child, so visit node 7.

The order of visiting nodes is: **1, 2, 3, 4, 5, 6, 7**.

### Mathematical Explanation
In-order traversal can be represented mathematically as:
\[
\text{InOrder}(T) = \text{InOrder}(T_{\text{left}}) \cup \{ \text{root} \} \cup \text{InOrder}(T_{\text{right}})
\]
Where:
- \( T \) is the tree.
- \( T_{\text{left}} \) is the left subtree.
- \( T_{\text{right}} \) is the right subtree.
- \( \{ \text{root} \} \) is the root node.

### Advantages of In-Order Traversal
1. **Sorted Output**: For a binary search tree, in-order traversal produces the nodes in ascending order.
2. **Simplicity**: The algorithm is straightforward and easy to implement using recursion.

### Conclusion
In-order traversal is a fundamental tree traversal method that uses binary recursion to visit nodes in a specific order. It is particularly useful for binary search trees, where it produces a sorted sequence of node values. The algorithm is simple, elegant, and efficient, making it a key tool in tree-based data structures.


The Euclidean algorithm is a classic method for finding the **greatest common divisor (GCD)** of two integers. The **tail-recursive** version of this algorithm is particularly efficient because it allows for **tail-call optimization**, which can prevent stack overflow and improve performance in languages that support this optimization.

### Euclidean Algorithm Overview
The Euclidean algorithm is based on the principle that the GCD of two numbers also divides their difference. The algorithm works as follows:
1. Given two integers \( a \) and \( b \), where \( a > b \), the GCD of \( a \) and \( b \) is the same as the GCD of \( b \) and \( a \mod b \).
2. This process is repeated until \( b \) becomes 0. At that point, \( a \) is the GCD.

### Tail Recursion
Tail recursion is a special form of recursion where the recursive call is the last operation in the function. This allows the compiler or interpreter to optimize the recursion by reusing the current stack frame, effectively turning the recursion into a loop.

### Tail-Recursive Euclidean Algorithm
Here’s how the tail-recursive version of the Euclidean algorithm works:

1. **Base Case**: If \( b = 0 \), the GCD is \( a \).
2. **Recursive Case**: Otherwise, compute \( a \mod b \) and make a recursive call with \( b \) and \( a \mod b \).

### Go Implementation
Here’s a Go implementation of the Euclidean algorithm using tail recursion:

```go
func Euclidean(a, b int) int {
    // Base case: if b is 0, return a
    if b ==  {
        return a
    }
    // Tail-recursive case: call Euclidean with b and a % b
    return Euclidean(b, a%b)
}
```

### Explanation of the Code
1. **Base Case**: When \( b = 0 \), the function returns \( a \), which is the GCD.
2. **Recursive Case**: The function calls itself with \( b \) and \( a \mod b \). This is the tail-recursive step because the recursive call is the last operation in the function.

### Example Walkthrough
Let’s compute the GCD of 48 and 18 using this algorithm:

1. **First Call**: \( a = 48 \), \( b = 18 \)
   - \( 48 \mod 18 = 12 \)
   - Recursive call: `Euclidean(18, 12)`

2. **Second Call**: \( a = 18 \), \( b = 12 \)
   - \( 18 \mod 12 = 6 \)
   - Recursive call: `Euclidean(12, 6)`

3. **Third Call**: \( a = 12 \), \( b = 6 \)
   - \( 12 \mod 6 = 0 \)
   - Recursive call: `Euclidean(6, 0)`

4. **Fourth Call**: \( a = 6 \), \( b = 0 \)
   - Base case reached, return \( 6 \)

The GCD of 48 and 18 is **6**.

### Advantages of Tail Recursion
1. **Efficiency**: Tail recursion can be optimized into a loop, reducing the overhead of function calls.
2. **Stack Safety**: Tail recursion avoids stack overflow issues that can occur with deep recursion in languages that support tail-call optimization.

### Mathematical Explanation
The Euclidean algorithm is based on the following mathematical property:
\[
\gcd(a, b) = \gcd(b, a \mod b)
\]
This property is used repeatedly until \( b = 0 \), at which point \( a \) is the GCD.

### Conclusion
The tail-recursive Euclidean algorithm is an efficient and elegant way to compute the GCD of two integers. It leverages the properties of modular arithmetic and tail recursion to provide a solution that is both simple and performant.


left encryption     right encryption

Diffusion    p-c
Confusion    k-c

transformation  : swap the left and right encryption -> make it hard to predict by compare p and c
substition:  exo  with the key -> make it hard to predict by compare k and c
sub

In abstract algeba

Groups
Abelian Groups
Rings
Commutative Rings
Integral Domains
Fields  Finite fields

elements    operations      groups
Rules : 


Project Report: K8s Autoscaling Project
This report summarizes the milestones, initial plan, and changes based on the project progress documented in the provided Excel file. The project focuses on implementing autoscaling in Kubernetes (K8s) using KEDA (Kubernetes Event-Driven Autoscaling) and integrating it with PredictKube for intelligent scaling decisions.

1. Initial Plan
The initial plan was to:

Set up a local Kubernetes cluster using MicroK8s.

Deploy KEDA for event-driven autoscaling.

Use Prometheus and Grafana for monitoring and observability.

Develop a sample web service (initially using Nginx) for load testing.

Use Vegeta for load testing and observe the scaling behavior.

Integrate PredictKube as a trigger for KEDA to enable intelligent scaling based on predictive metrics.

Test and debug the system to ensure smooth scaling under different load conditions.

2. Milestones
Week 1-3: Setup and Initial Testing
Week 1:

Set up MicroK8s with 2 CPUs and 4GB memory.

Deployed KEDA, Prometheus, and Grafana.

Created an Nginx service for testing and exposed it.

Conducted initial load testing using Vegeta.

Created a KEDA ScaledObject with Prometheus as a trigger.

Discussed the plan with the PredictKube team and Professor Amin.

Week 2:

Replaced Nginx with a custom Golang service (using the Gin framework) with a 5-minute startup time.

Conducted load testing and observed scaling behavior.

Started integrating PredictKube but faced issues with the API key.

Week 3:

Debugged PredictKube key invalidation issues.

Researched Vegeta load testing tool and integrated it with Prometheus.

Faced stability issues with the local K8s cluster under high load (above 1000 RPS).

Week 4-6: Migration to AWS EKS and Advanced Testing
Week 4:

Migrated the K8s cluster to AWS EKS for better stability.

Deployed KEDA and PredictKube in AWS EKS.

Researched integrating Vegeta metrics with Prometheus and Grafana.

Debugged service crashes in EKS and replaced the victim service.

Week 5:

Set up Prometheus metrics and Grafana dashboards to monitor HTTP requests.

Used Kong Ingress to expose the victim service for better stability during load testing.

Confirmed that PredictKube could be used for free in the project setup.

Week 6:

Presented AWS EKS and PredictKube options to Professor Amin.

Set up HTTP request metrics in Prometheus and Grafana.

Explored nested metrics to distinguish legitimate and DDoS traffic.

Tested scaling behavior with PredictKube settings.

Week 7-9: Debugging and Advanced Integration
Week 7:

Implemented nested thresholds to distinguish legitimate and DDoS requests.

Debugged PredictKube gRPC connection failures.

Switched to Prometheus as a trigger for scaling and observed successful scaling.

Updated Prometheus queries to prevent scaling on 3xx status codes.

Week 8:

Researched KEDA scalers, including PredictKube and Prometheus.

Used Telepresence to debug PredictKube gRPC connection failures.

Traced KEDA source code to understand scaling behavior.

Observed that KEDA's ScaledObject controller queries Prometheus API for metrics.

Week 9:

Experimented with running ScaledObject with multiple triggers (PredictKube and Prometheus).

Proxied Prometheus server locally to debug PredictKube failures.

Compared PredictKube and Prometheus triggers and identified PredictKube's limitations.

Week 10-11: Final Testing and Future Plans
Week 10:

Debugged KEDA operator and tested rate limiting.

Successfully scaled the target deployment using PredictKube trigger.

Continued researching KEDA and K8s libraries for improvements.

Week 11:

Planned to create a separate AWS account for testing.

Considered subscribing to PredictKube's paid service for better support.

Aimed to integrate a custom predictive model with KEDA and compare it with PredictKube.

3. Changes from Initial Plan
Migration to AWS EKS:

The local K8s cluster was unstable under high load, so the project migrated to AWS EKS for better performance and stability.

Replacement of Nginx with Custom Golang Service:

The initial Nginx service was replaced with a custom Golang service to simulate a 5-minute startup time and better control over behavior.

Integration of Kong Ingress:

Initially, port forwarding was used to expose services, but it was unstable under high load. Kong Ingress was introduced for better stability.

Switching from PredictKube to Prometheus Trigger:

Due to persistent issues with PredictKube's gRPC connection, the project temporarily switched to using Prometheus as the primary trigger for scaling.

Exploration of Nested Metrics:

The project explored using nested metrics to distinguish between legitimate and DDoS traffic, which was not part of the initial plan.

Debugging and Code Exploration:

Significant time was spent debugging KEDA and PredictKube, including tracing source code and experimenting with multiple triggers.

Future Plans for Paid PredictKube Service:

The team considered subscribing to PredictKube's paid service for better support and features, which was not part of the initial plan.

4. Key Learnings and Challenges
Key Learnings:
KEDA's Flexibility: KEDA supports multiple triggers (e.g., Prometheus, PredictKube) and can be customized for different scaling scenarios.

Importance of Observability: Prometheus and Grafana were critical for monitoring and debugging scaling behavior.

Cloud Migration: Migrating to AWS EKS improved stability and performance under high load.

Challenges:
PredictKube Integration: Persistent issues with PredictKube's gRPC connection delayed progress.

Local Cluster Limitations: The local K8s cluster was unstable under high load, necessitating a move to AWS EKS.

Complexity of Nested Metrics: Implementing nested metrics to distinguish traffic types was more challenging than anticipated.

5. Conclusion
The project successfully implemented autoscaling in Kubernetes using KEDA and integrated it with Prometheus for monitoring. While PredictKube integration faced challenges, the team explored alternative solutions and gained valuable insights into KEDA's scaling mechanisms. Future work includes resolving PredictKube issues, integrating a custom predictive model, and comparing it with PredictKube's performance.

This report highlights the project's progress, challenges, and adaptations, providing a clear overview of the milestones and changes from the initial plan.